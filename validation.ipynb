{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(\n",
    "        data_series,\n",
    "        predict_fn,\n",
    "        training_window=8760,\n",
    "        forecast_horizon=168,\n",
    "        expanding=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    General walk-forward validation.\n",
    "    \n",
    "    data_series: pandas DataFrame with target as first column\n",
    "    predict_fn: callable with signature predict_fn(train_data) -> np.array of length forecast_horizon\n",
    "    expanding: if True, uses expanding window; if False, uses sliding window\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "    all_fold_rmse = []\n",
    "    all_fold_mae = []\n",
    "\n",
    "    if expanding:\n",
    "        num_folds = (len(data_series) - training_window) // forecast_horizon\n",
    "    else:\n",
    "        num_folds = (len(data_series) - training_window - forecast_horizon) // forecast_horizon\n",
    "\n",
    "    print(f\"Total folds: {num_folds}\")\n",
    "    print(f\"Training window: {training_window} hours\")\n",
    "    print(f\"Forecast horizon: {forecast_horizon} hours\")\n",
    "    print(f\"Mode: {'Expanding' if expanding else 'Sliding'} window\\n\")\n",
    "\n",
    "    for fold in range(num_folds):\n",
    "        if expanding:\n",
    "            train_start = 0\n",
    "            train_end = training_window + fold * forecast_horizon\n",
    "        else:\n",
    "            train_start = fold * forecast_horizon\n",
    "            train_end = train_start + training_window\n",
    "\n",
    "        val_start = train_end\n",
    "        val_end = val_start + forecast_horizon\n",
    "\n",
    "        if val_end > len(data_series):\n",
    "            break\n",
    "\n",
    "        print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "\n",
    "        train_data = data_series.iloc[train_start:train_end]\n",
    "        val_data = data_series.iloc[val_start:val_end]\n",
    "\n",
    "        predictions = predict_fn(train_data)\n",
    "        actuals = val_data.iloc[:, 0].values  # target column only\n",
    "\n",
    "        assert len(predictions) == forecast_horizon, \\\n",
    "            f\"predict_fn must return {forecast_horizon} predictions, got {len(predictions)}\"\n",
    "\n",
    "        fold_rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "        fold_mae = mean_absolute_error(actuals, predictions)\n",
    "\n",
    "        all_fold_rmse.append(fold_rmse)\n",
    "        all_fold_mae.append(fold_mae)\n",
    "        all_predictions.extend(predictions)\n",
    "        all_actuals.extend(actuals)\n",
    "\n",
    "        print(f\"  Fold RMSE: {fold_rmse:.2f}, MAE: {fold_mae:.2f}\\n\")\n",
    "\n",
    "    overall_rmse = np.sqrt(mean_squared_error(all_actuals, all_predictions))\n",
    "    overall_mae = mean_absolute_error(all_actuals, all_predictions)\n",
    "\n",
    "    print(f\"Overall RMSE: {overall_rmse:.2f}\")\n",
    "    print(f\"Overall MAE: {overall_mae:.2f}\")\n",
    "    print(f\"Average Fold RMSE: {np.mean(all_fold_rmse):.2f} (+/- {np.std(all_fold_rmse):.2f})\")\n",
    "    print(f\"Average Fold MAE: {np.mean(all_fold_mae):.2f} (+/- {np.std(all_fold_mae):.2f})\")\n",
    "\n",
    "    return all_predictions, all_actuals, all_fold_rmse, all_fold_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SequenceDataset (multivariate) ---\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length=24):\n",
    "        \"\"\"\n",
    "        data: numpy array of shape (n_samples, n_features)\n",
    "        Target is assumed to be the first column.\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data = torch.FloatTensor(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.sequence_length]         # (sequence_length, n_features)\n",
    "        y = self.data[idx + self.sequence_length, 0]          # target column only\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# --- LSTM ---\n",
    "def make_lstm_predict_fn(model_class, forecast_horizon, sequence_length=24,\n",
    "                          hidden_size=64, num_layers=2, learning_rate=0.001,\n",
    "                          num_epochs=50, batch_size=16, **model_kwargs):\n",
    "    def predict_fn(train_data):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        n_features = train_data.shape[1]\n",
    "\n",
    "        # Standardize each feature using training statistics\n",
    "        train_mean = train_data.mean()\n",
    "        train_std = train_data.std()\n",
    "        train_scaled = (train_data.values - train_mean.values) / train_std.values\n",
    "\n",
    "        # Train\n",
    "        dataset = SequenceDataset(train_scaled, sequence_length)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_class(\n",
    "            input_size=n_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            output_size=1,\n",
    "            **model_kwargs\n",
    "        ).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(X_batch), y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Autoregressive prediction with frozen exogenous features\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        input_seq = torch.FloatTensor(train_scaled[-sequence_length:]).unsqueeze(0).to(device)\n",
    "        # Shape: (1, sequence_length, n_features)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(forecast_horizon):\n",
    "                output = model(input_seq)\n",
    "                next_val = output.squeeze().item()\n",
    "                predictions.append(next_val)\n",
    "\n",
    "                # Build next timestep\n",
    "                next_step = input_seq[:, -1, :].clone()  # copy last timestep\n",
    "                next_step[:, 0] = next_val               # update target (price)\n",
    "                # columns 1 onwards are frozen at their last known real value\n",
    "\n",
    "                input_seq = torch.cat([input_seq[:, 1:, :], next_step.unsqueeze(1)], dim=1)\n",
    "\n",
    "        # Un-standardize target only (column 0)\n",
    "        predictions = np.array(predictions) * train_std.iloc[0] + train_mean.iloc[0]\n",
    "        return predictions\n",
    "\n",
    "    return predict_fn"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
